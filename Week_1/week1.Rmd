---
title: "Computing the LASSO"
author: "Ivan"
date: "2/6/2023"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Question 1:
### Part a

Will the lasso always select more variables than the adaptive lasso? It depends. If we are assuming orthogonal design and a "medium" $\lambda$ across both estimators, then the lasso and adaptive lasso choose the same variables. If we have a "small" $\lambda$ across both estimators, then the adaptive lasso penalizes heavier less irrelevant variables and decreases the bias for more relevant variables. For a large value of $\lambda$, the AL actually chooses **less** variables than the normal lasso

```{r lasso vs AL, echo=FALSE, out.width = '35%', fig.align = "center"}
knitr::include_graphics("/Users/Ivan/Desktop/Areas/UM/CLASSES/YEAR_2/period_4/BIg_Data/computing_the_lasso/lassovsAL.png")
```

### Part b

Do we always prefer the adaptive lasso over the lasso? Well, the adaptive lasso always gives us all oracle properties, while the normal lasso can never do this. But perhaps we are not interested in variable selection, but in prediction. In addition, as noted in Zhou (2006), when we have a low signal to noise ratio (relatively more noise than signal), then the lasso performs better than the adaptive lasso.

### Part c

Do we prefer information criteria or cross validation? It depends on the use case. Using cross validation, we lose some information because we dedicate some portions to training and testing. But this could be much better in a prediction setting. Information criteria may be better when we have less samples to choose from.

### Part d
$$y_i = \sum_{j=1}^p \beta_j x_{i,j} + \varepsilon_i$$
where $\beta_j = \alpha^j$ for some $0 < \alpha < 1$. Is $\beta$ weakly sparse?

Then we know that $\alpha$ tends to zero at some rate, depending on the form of $\alpha$. Suppose we had strong sparsity. Then we would have
$$||\beta_j||_0^0 = \sum_{j=1}^k |\beta_j|^0 = \sum_{j=1}^k \mathbb{1}(|\beta_j| > 0) = o\left(\sqrt{\frac{n}{\ln p}}\right)$$
This then means that the cardinality of $\alpha^j$ would need to be of smaller order than $\sqrt{\frac{n}{\ln p}}$. Not true. Now suppose weak sparsity.
$$||\mathcal{\beta_j}||_1 = \sum_{j=1}^p |\beta_j| = o\left(\sqrt{\frac{n}{\ln p}}\right)$$
Then we have the sum of the absolute values should not be too large. This is the case when
$$\sum_{j=1}^p |\beta_j| = \sum_{j=1}^p |\alpha^j| = o\left(\sqrt{\frac{n}{\ln p}}\right) \Rightarrow  \sqrt{\frac{\ln p}{n}}\sum_{j=1}^p |\alpha^j| \to 0$$
Because even as $p$ increases, we have the sum converging to zero. Unlike in cardinality where we have a sum of ones.

We have a geometric sum that converges to a finite value

## Question 2:
### Part a
$$\hat \beta^* = \underset{\beta}{\arg\min} \left(||\mathbf{y} - \mathbf{X} \beta||_2^2/n + \lambda_1 ||\beta||_1 + \lambda_2 ||\beta||_2^2\right)$$
```{r EN FOC, echo=FALSE, out.width = '70%', fig.align = "center"}
knitr::include_graphics("/Users/Ivan/Desktop/Areas/UM/CLASSES/YEAR_2/period_4/BIg_Data/computing_the_lasso/week1_2a.jpeg")
```

### Part b

```{r EN derivation, echo=FALSE, out.width = '80%', fig.align = "center"}
knitr::include_graphics("/Users/Ivan/Desktop/Areas/UM/CLASSES/YEAR_2/period_4/BIg_Data/computing_the_lasso/week1_2b.png")
```

### Part c

Why do we scale EN by $(1 + \lambda/2)$? We do this to improve **prediction performance**. This reduces the bias in the naive elastic net and offsets the double shrinkage. Double shrinkage does not help reduce the variance and introduces unnecessary extra bias. We end up with the lasso under orthogonality!

### Part d

```{r EN distribution, echo=FALSE, out.width = '80%', fig.align = "center"}
knitr::include_graphics("/Users/Ivan/Desktop/Areas/UM/CLASSES/YEAR_2/period_4/BIg_Data/computing_the_lasso/week1_2d.jpeg")
```

### Part e

i. For the Naive elastic net, we are looking for consistent estimation of $\hat \beta_{NEN}$, such that for every $\beta$ and $\varepsilon > 0$,
$$\lim_{n \to \infty} \mathbb{P}\left(\left| \hat \beta_{NEN} - \beta\right| < \varepsilon\right) = 1 \quad \text{or} \quad \lim_{n \to \infty} \mathbb{P}\left(\left| \hat \beta_{NEN} - \beta\right| \geq \varepsilon\right) = 0$$
For fixed $p$ and $n \to \infty$, we need
$$\frac{|\hat \beta_{OLS}|}{1 + \lambda_2} - \frac{\lambda_1/2}{1 + \lambda_2} \overset{p}{\to} \beta$$
In other words,
$$\lambda_1 \to 0, \quad \lambda_2 \to 0$$

For both $p \to \infty$ and $n \to \infty$, we need assumptions on sparsity. Suppose we have weak sparsity, or in other words,
$$||\mathcal{\beta}||_1 = \sum_{j=1}^p |\beta_j| = o\left(\sqrt{\frac{n}{\ln p}}\right)$$
Then our $\lambda_1$ needs to grow at a rate opposite of this, $\sqrt{\frac{\ln p}{n}}$. By similar logic, we can take the $\ell_2$-norm as

$$||\mathcal{\beta}||_2^2 = \sum_{j=1}^p \beta_j^2 = o\left(\frac{n}{\ln p}\right)$$
Then our $\lambda_2$ needs to grow at a rate opposite of this, $\frac{\ln p}{n}$. 

For the normal elastic net, we multiply the whole parameter by $(1 + \lambda_2)$, so in order to correct for this, our $\lambda_1$ now takes the form $\lambda_1 + \lambda_2 \lambda_1$ and our $\lambda_2$ takes the form $\lambda_2 + \lambda_2^2$. The higher polynomial takes over, which in the first case is $\lambda_1 \lambda_2$ and the second case is $\lambda_2^2$. We then have to cancel the terms to grow at an opposite rate, solving the system
$$\lambda_1 \lambda_2 = \sqrt{\frac{n}{\ln p}}$$
$$\lambda_2^2 = \frac{n}{\ln p}$$
$$\lambda_2 = \sqrt{\frac{n}{\ln p}}, \quad \lambda_1 = 1$$
So we only need our $\lambda_2$ to grow at a rate of $\sqrt{\frac{n}{\ln p}}$ and our $\lambda_1$ can grow at a constant rate.

- $\ell_2$-norm going to zero doesn't mean our l1 norm also goes to zero
- p increases to infinity, we need $\sqrt{p} * \ell_2$-norm to also to go zero for our $\ell_1$-norm to go to zero
- Lasso is consistent as long as penalty goes to zero as n to infinity



### Part f

```{r EN variable screening, echo=FALSE, out.width = '70%', fig.align = "center"}
knitr::include_graphics("/Users/Ivan/Desktop/Areas/UM/CLASSES/YEAR_2/period_4/BIg_Data/computing_the_lasso/week1_2f.jpeg")
```
$\lambda_1$ would then need to go to zero, weak $\ell_1$ penalty

### Part g

```{r EN variable selection, echo=FALSE, out.width = '70%', fig.align = "center"}
knitr::include_graphics("/Users/Ivan/Desktop/Areas/UM/CLASSES/YEAR_2/period_4/BIg_Data/computing_the_lasso/week1_2g.png")
```

## Question 3

Split cases where $q = 0$ because $0^0$ is UNDEFINED. Need to check which one is smallest. This gives us a **hard threshold** condition. We do not shrink anymore. We don't incur a bias, directly end up with the sample mean. Disadvantage is instability, and is computationally complex. Check for all different variables, which gives lowest RSS.
```{r optimization, echo=FALSE, out.width = '60%', fig.align = "center"}
knitr::include_graphics("/Users/Ivan/Desktop/Areas/UM/CLASSES/YEAR_2/period_4/BIg_Data/computing_the_lasso/week1_3.jpeg")
```


## Question 4

`lambda.1se` can be used in order to impose stricter penalization.
We are interested in characteristics of MLB players and salaries. Salary will be our response variable. Load the data and perform some preprocessing
```{r Loadings, echo = TRUE, results = 'hide', warning=FALSE, message=FALSE}
set.seed(20230209)
library(ISLR2)
library(glmnet)
x <- model.matrix(Salary ~ ., data = na.omit(Hitters))[,-1]
y <- na.omit(Hitters$Salary)
```
`model.matrix()` changes letter values to numerical dummies, and removes the salary column from the matrix.
Use glmnet to perform lasso regression and select tuning parameter using 10-fold cross validation. Lambda can be a value from $10^{-2}$ to $10^{10}$
Error metric is the MSE.

```{r lasso and cv}
cv.lasso <- cv.glmnet(x,y,alpha=1)
best.lasso.lambda <- cv.lasso$lambda.min
lasso.model <- glmnet(x,y,alpha=1, lambda = best.lasso.lambda)
coef(lasso.model)
```
Here, we see `HmRun`, `Runs`, `RBI`, `CAtBat`, `CHits`, `NewLeagueN` are all shrunk to zero. The most influential seems to be new league, which is
1 if it is "N". What we notice here is that redundant information is kicked out, i.e. `NewLeagueN` and `LeagueN` measure the same, but `LeagueN` is kept in. This may change with Elastic Net. Additionally, `Hits` is more influential as well as `Walks` in determining salary.

```{r Ridge}
cv.ridge <- cv.glmnet(x,y,alpha=0)
best.ridge.lambda <- cv.ridge$lambda.min
ridge.model <- glmnet(x,y,alpha=0,lambda = best.ridge.lambda)
coef(ridge.model)
```
Here, the ridge regression keeps all coefficients instead of kicking some out.

```{r Elastic Net}
cv.EN <- cv.glmnet(x,y,alpha=0.5)
best.EN.lambda <- cv.EN$lambda.min
EN.model <- glmnet(x,y,alpha=0.5,lambda = best.EN.lambda)
coef(EN.model)
```

Elastic Net kicks out more variables than the ridge regression, but not more than lasso. This could be because there is quite a bit of correlation between each of the parameters. Here, we see that `Walks` influences `Salary` quite a bit and `Hits` is also a big indicator for the salary. Next we do adaptive lasso with Ridge Regression to create the weights vector.

```{r Adaptive Lasso}
w3 <- 1/abs(matrix(coef(cv.ridge, s=cv.ridge$lambda.min)[, 1][2:(ncol(x)+1)]))^1 ## Using gamma = 1. Select the first column, change to a vector
cv.AL <- cv.glmnet(x,y,alpha = 1, penalty.factor=w3)
best.AL.lambda <- cv.AL$lambda.min
AL.model <- glmnet(x,y,alpha=1, lambda = best.AL.lambda, penalty.factor = w3)
coef(AL.model)
```
Although hard to see, we have multiple values that are "kicked out", or have marginal effect, and others, such as `Years` and `DivisionW` and `LeagueN` which stay influential. This may be the best model yet.

Now we use `HDeconometrics` to perform a lasso regression and select a tuning parameter using AIC and BIC

```{r econometrics}
library(HDeconometrics)
bic.lasso <- ic.glmnet(x,y,crit = "bic", alpha = 1)
coef(bic.lasso)
aic.lasso <- ic.glmnet(x,y,crit = "aic", alpha = 1)
coef(aic.lasso)
```
Here, we see that BIC is more sparse. The ones chosen by BIC are also chosen by cross validation. AIC does not shrink as many variables to zero. In fact, AIC chooses the same values as cross validation. The only difference is in the coefficients slightly.

Now we see how well we can predict. The difference here is we ignore portions of the data set instead of above where we included everything
```{r prediction}
train <- sample(1:nrow(x), nrow(x)/2)
test <- (-train)
y.test <- y[test]

cv.out <- cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.out)
bestlam <- cv.out$lambda.min
lasso.mod <- glmnet(x[train,],y[train],alpha = 1, lambda = bestlam)
lasso.pred <- predict(lasso.mod, s = bestlam, newx = x[test,])
mean((lasso.pred - y.test)^2)

ridge.cv.out <- cv.glmnet(x[train,],y[train],alpha=0)  # Ridge
ridge.bestlam <- ridge.cv.out$lambda.min
ridge.mod <- glmnet(x[train,],y[train],alpha = 0, lambda = ridge.bestlam)
ridge.pred <- predict(ridge.mod, s = ridge.bestlam, newx = x[test,])
mean((ridge.pred - y.test)^2)

EN.cv.out <- cv.glmnet(x[train,],y[train],alpha=0.5)  # EN
EN.bestlam <- EN.cv.out$lambda.min
EN.mod <- glmnet(x[train,],y[train],alpha = 0.5, lambda = EN.bestlam)
EN.pred <- predict(EN.mod, s = EN.bestlam, newx = x[test,])
mean((EN.pred - y.test)^2)

w3 <- 1/abs(matrix(coef(cv.ridge, s=cv.ridge$lambda.min)[, 1][2:(ncol(x)+1)]))^1  # Use same weight as before
AL.cv.out <- cv.glmnet(x[train,],y[train],alpha=1,penalty.factor=w3)  # AL
AL.bestlam <- AL.cv.out$lambda.min
AL.mod <- glmnet(x[train,],y[train],alpha = 1, penalty.factor = w3)
AL.pred <- predict(AL.mod, s = AL.bestlam, newx = x[test,])
mean((AL.pred - y.test)^2)
```

Our best model for prediction in this case is the Ridge Regression, with a mean squared prediction error of 80993.28.

## Question 5

Load and clean the data

```{r kaggle download}
housing.data <- read.csv("/Users/Ivan/Desktop/Areas/UM/CLASSES/YEAR_2/period_4/BIg_Data/computing_the_lasso/housing.csv")
housing.data$ocean_proximity = as.factor(housing.data$ocean_proximity)  # Change from text-based to factor variable
housing.data <- housing.data[housing.data$ocean_proximity != "ISLAND", ]  # Remove "ISLAND" as it could be interpretted as outlier
housing.data <- na.omit(housing.data)  # Delete observations with missing values
housing.data <- housing.data[housing.data$median_house_value < 500000, ]  # Remove value larger than 500,000
```

Variables which are best treated as categorical:

- Number of rooms
- Population
- Number of bedrooms

These are chosen because they are the most right skewed given a histogram of counts.

```{r histogram}
hist(housing.data$total_rooms, breaks = 20, main = "total_rooms", border="darkorange", col="dodgerblue")
hist(housing.data$total_bedrooms, breaks = 20, main = "total_bedrooms", border="darkorange", col="dodgerblue")
hist(housing.data$population, breaks = 20, main = "population", border="darkorange", col="dodgerblue")
```

Dummies are split into low, average, and high, with dummies created for only low and average to prevent collinearity. Thresholds are chosen from the first third tercile

```{r dummies}
tercile.rooms <- quantile(housing.data$total_rooms, c(0:3/3))
housing.data$lowrooms <- ifelse(housing.data$total_rooms < tercile.rooms[2],1,0)
housing.data$midrooms <- ifelse(housing.data$total_rooms >= tercile.rooms[2] & housing.data$total_rooms < tercile.rooms[3],1,0)

tercile.bedrooms <- quantile(housing.data$total_bedrooms, c(0:3/3))
housing.data$lowbedrooms <- ifelse(housing.data$total_bedrooms < tercile.bedrooms[2],1,0)
housing.data$midbedrooms <- ifelse(housing.data$total_bedrooms >= tercile.bedrooms[2] & housing.data$total_bedrooms < tercile.bedrooms[3],1,0)

tercile.pop <- quantile(housing.data$population, c(0:3/3))
housing.data$lowpopulation <- ifelse(housing.data$population < tercile.pop[2],1,0)
housing.data$midpopulation <- ifelse(housing.data$population >= tercile.pop[2] & housing.data$population < tercile.pop[3],1,0)
```

We also add interactions and higher powers. They are shown in the following formula

```{r interactions}
ff <- median_house_value ~ . + .^2 + I(housing_median_age^2)
housing.X <- model.matrix(ff,housing.data)[,-1]
housing.y <- housing.data$median_house_value
dim(housing.X)
```
We now have 166 variables and 19443 observations. We do our normal analysis as follows

```{r housing lasso variants, echo=TRUE}
train.h <- sample(1:nrow(housing.X), nrow(housing.X)/2)
test.h <- (-train.h)
y.test.h <- housing.y[test.h]

cv.housing.lasso <- cv.glmnet(housing.X[train.h,],housing.y[train.h],alpha=1)
housing.lambda.lasso <- cv.housing.lasso$lambda.min
housing.lasso <- glmnet(housing.X[train.h,],housing.y[train.h],alpha=1, lambda = housing.lambda.lasso)
housing.lasso.pred <- predict(housing.lasso, s = housing.lambda.lasso, newx = housing.X[test.h,])
sqrt(mean((housing.lasso.pred - y.test.h)^2))

cv.housing.ridge <- cv.glmnet(housing.X[train.h,],housing.y[train.h],alpha=0)
housing.lambda.ridge <- cv.housing.ridge$lambda.min
housing.ridge <- glmnet(housing.X[train.h,],housing.y[train.h],alpha=0, lambda = housing.lambda.ridge)
housing.ridge.pred <- predict(housing.ridge, s = housing.lambda.ridge, newx = housing.X[test.h,])
sqrt(mean((housing.ridge.pred - y.test.h)^2))

cv.housing.EN <- cv.glmnet(housing.X[train.h,],housing.y[train.h],alpha=0.5)
housing.lambda.EN <- cv.housing.EN$lambda.min
housing.EN <- glmnet(housing.X[train.h,],housing.y[train.h],alpha=0.5, lambda = housing.lambda.EN)
housing.EN.pred <- predict(housing.EN, s = housing.lambda.EN, newx = housing.X[test.h,])
sqrt(mean((housing.EN.pred - y.test.h)^2))

w3.h <- 1/abs(matrix(coef(cv.housing.ridge, s=cv.housing.ridge$lambda.min)[, 1][2:(ncol(housing.X)+1)]))^1
cv.housing.AL <- cv.glmnet(housing.X[train.h,],housing.y[train.h],alpha=1,penalty.factor=w3.h)  # AL
housing.lambda.AL <- cv.housing.AL$lambda.min
housing.AL <- glmnet(housing.X[train.h,],housing.y[train.h],alpha = 1, penalty.factor = w3.h)
housing.AL.pred <- predict(housing.AL, s = housing.lambda.AL, newx = housing.X[test.h,])
sqrt(mean((housing.AL.pred - y.test.h)^2))

# Group Lasso
library(gglasso)

# Split groups based on names: total 16 groups
GL.groups <- numeric(ncol(housing.X))
col.names <- colnames(housing.X)
group_names <- c("long", "lati", "housing", "total_r", "total_b", 
                 "pop", "househ", "median", "ocean", "lowrooms", 
                 "midrooms", "lowb", "midb", "lowp", "midp")
group_values <- 1:length(group_names)

for (i in 1:length(group_names)) {
  GL.groups[grep(group_names[i], col.names)] <- group_values[i]
}

# Take only 500 samples of housing because gglasso is slow
sample.housing <- housing.data[sample(nrow(housing.data), size = 500),]

# Split the data matrices once again and organize the grouping columns
sample.housing.X <- model.matrix(ff,sample.housing)[,-1]
sample.housing.X <- sample.housing.X[,GL.groups]
sample.housing.y <- sample.housing$median_house_value

# Split into train and test
sample.train <- sample(1:nrow(sample.housing.X), nrow(sample.housing.X)/2)
sample.test <- (-sample.train)
sample.y.test <- sample.housing.y[sample.test]

# Set vector of groups
GL.groups <- sort(GL.groups)-1
groups.c <- GL.groups
groups.c[groups.c == 1] <- 2
groups.c[groups.c == 0] <- 1

# Perform group lasso
cv.housing.GL <- cv.gglasso(sample.housing.X[sample.train,],sample.housing.y[sample.train], group = groups.c)
housing.lambda.GL <- cv.housing.GL$lambda.min
housing.GL <- gglasso(sample.housing.X[sample.train,],sample.housing.y[sample.train], lambda = housing.lambda.GL)
housing.GL.pred <- predict(housing.GL, s = housing.lambda.GL, newx = sample.housing.X[sample.test,])
sqrt(mean((housing.GL.pred - sample.y.test)^2))
```

So which variables matter most for predicting housing prices? 

- You fix other variables and change one
- Gives a marginal effect estimate
- Create counterfactual data, can be done with a function
- Take means, and then just vary the age
- Regress house price on age, people living in the area
